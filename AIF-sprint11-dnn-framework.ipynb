{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題提出時（2018/11/26）の所感\n",
    "sprint11では初めてTensorFlowのコードリーディングを行い、さわってみました。確かにスクラッチよりもずっと簡単にNNを組めるので大変便利だと感じました。まだ使い方に慣れていないので、おいおい使って行きたいと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.この課題の目的\n",
    "フレームワークのコードを読めるようにする  \n",
    "フレームワークを習得し続けられるようになる  \n",
    "理論を知っている範囲をフレームワークで動かす  \n",
    "\n",
    "【目的としないこと】  \n",
    "各フレームワークの細かいノウハウの習得  \n",
    "デプロイなど動かしてみたその先  \n",
    "新たに見かけた理論の詳細な理解  \n",
    "\n",
    "# 2.進め方\n",
    "コードリーディング（1日目）  \n",
    "いくつかのデータセットをフレームワークで扱う（1日目〜2日目）  \n",
    "\n",
    "# 3.コードリーディング\n",
    "TensorFLowによって2値分類を行うサンプルコードを載せた。今回はこれをベースにして進める。\b\n",
    "tf.kerasやtf.estimatorなどの高レベルAPIは使用していない。既にNumPyによるスクラッチを行っているから、フレームワークも低レベルなところから見ていくことにする。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スクラッチを振り返る\n",
    "sprint9,10で行ったスクラッチを振り返り、ディープラーニングを実装するためにはどのようなものが必要だったかを列挙する。\n",
    "\n",
    "・トレイナークラス・・・Fit/Predict  \n",
    "・最適化クラス（Optimizer）・・・SGD  \n",
    "・活性化関数クラス（Activationfunction） ・・・Tanh,sigmoid <backward>  \n",
    "・損失関数クラス（Metrics）（測定基準）：MSE/交差エントロピー <backward>  \n",
    "・全結合層クラス・・・np.dot  、<backward>  \n",
    "・初期化クラス・・・重み・バイアス  \n",
    "・エポック、ミニバッチ  \n",
    "・前処理  \n",
    "    \n",
    "    \n",
    " それらがフレームワークにおいてはどのように実装されるかを確認していく。\n",
    " \n",
    " 【調査結果】\n",
    "NumPy  ：  np.dot\n",
    "TensorFlow  :  tf.matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#データの取り込み\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import  train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "# Irisデータセットの用意\n",
    "iris = pd.read_csv(\"/Users/tsuneo/kaggle/IrisSpecies/Iris.csv\")\n",
    "# 型の確認\n",
    "iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  目的変数Speciesの中から2つに限定する\n",
    "iris = iris.query('Species in [\"Iris-versicolor\", \"Iris-virginica\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# irisのデータを説明変数、目的変数に分割する\n",
    "# X = iris[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]].values\n",
    "# y = iris[[\"Species\"]]\n",
    "# print(type(y))\n",
    "# y[\"Species\"] = y[\"Species\"].map({'Iris-versicolor':0, 'Iris-virginica':1})[:,None]\n",
    "# print(type(y))\n",
    "# X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サンプルコード\n",
    "以下のサンプルコードを見て、先ほど列挙した「ディープラーニングを実装するために必要なもの」がTensorFlowではどう実装されているかを確認する。\n",
    "それを簡単に言葉でまとめておくこと。単純な一対一の対応であるとは限らない。\n",
    "\n",
    "＊バージョン1.5から1.12の間では動作を確認済みである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 1.7807, val_loss : 7.4728, acc : 0.750, val_acc : 0.438\n",
      "Epoch 1, loss : 0.0000, val_loss : 0.8265, acc : 1.000, val_acc : 0.812\n",
      "Epoch 2, loss : 0.0014, val_loss : 4.3522, acc : 1.000, val_acc : 0.750\n",
      "Epoch 3, loss : 1.5344, val_loss : 7.7090, acc : 0.750, val_acc : 0.500\n",
      "Epoch 4, loss : 0.1619, val_loss : 6.5967, acc : 1.000, val_acc : 0.750\n",
      "Epoch 5, loss : 2.0780, val_loss : 8.6058, acc : 0.750, val_acc : 0.500\n",
      "Epoch 6, loss : 0.0013, val_loss : 5.6772, acc : 1.000, val_acc : 0.750\n",
      "Epoch 7, loss : 1.5678, val_loss : 7.9787, acc : 0.750, val_acc : 0.500\n",
      "Epoch 8, loss : 0.0000, val_loss : 3.9957, acc : 1.000, val_acc : 0.812\n",
      "Epoch 9, loss : 0.2258, val_loss : 6.7578, acc : 0.750, val_acc : 0.562\n",
      "test_acc : 0.700\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類するコード\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# データセットの読み込み\n",
    "dataset_path =\"/Users/tsuneo/kaggle/IrisSpecies/Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._counter = 0\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            self._counter = 0\n",
    "            raise StopIteration()\n",
    "\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # 全結合層っぽい\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    \n",
    "    # 活性化関数reluのようだ\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # 全結合\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    \n",
    "    # 活性化関数relu\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # 全結合のままで出力しているので、恒等関数で対応している意味合い？\n",
    "    # →違う。出力層の活性化関数はsigmoidである。誤差逆伝搬のコードは★１\n",
    "    # 誤差逆伝搬ではない、予測値を算出する処理は★2\n",
    "    # つまり、誤差逆伝搬の「計算グラフ」では、★1と★２に枝分かれしている\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数\n",
    "# reduce_meanは、np.meanと等価のようだ\n",
    "# シグモイド＋交差エントロピーの組み合わせ.サンプル毎の損失をだすため、reduce_meanしている\n",
    "# ★１\n",
    "# ◼️◼️◼️◼️◼️ループで呼び出している箇所\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "\n",
    "# 最適化手法\n",
    "# AdamOptimizer　：Adamアルゴリズムを実装するオプティマイザ\n",
    "# 重みを更新する処理\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# ◼️◼️◼️◼️◼️ループで呼び出している箇所\n",
    "# loss更新によって最小化する操作を追加し、var_list\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果 -0.5は閾値　★2\n",
    "tmp1 = tf.sign(Y - 0.5) # debug\n",
    "tmp2 = tf.sigmoid(logits)\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "\n",
    "# 指標値計算\n",
    "# ◼️◼️◼️◼️◼️ループで呼び出している箇所\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    # このrunでは、initだけを実行している。\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        #print(X_train.shape[])\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        \n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            \n",
    "            # ミニバッチごとにループ\n",
    "            # \n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #aa  = sess.run(correct_pred, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #bb = sess.run(correct_pred, feed_dict={X: mini_batch_x, Y: mini_batch_y}) #debug\n",
    "            #print(acc)\n",
    "            #cc = sess.run(tmp2, feed_dict={X: mini_batch_x, Y: mini_batch_y})  #debug\n",
    "            #print(\"correct_pred\", bb.dtype)\n",
    "            #print(\"tf.sigmoid(logits).shape\",cc.shape)\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.他のデータセットへの適用\n",
    "これまでに扱ってきた小さなデータセットがいくつかある。これらに対して学習・推定を行うニューラルネットワークを作成する。\n",
    "どれも簡単なデータセットではあるが、新しいデータに対してフレームワークを動かすということは今後の基本になる部分である。  \n",
    "Iris（3種類全ての目的変数を使用）  \n",
    "MNIST  \n",
    "house price  \n",
    "どのデータセットもtrain, val, testの3種類に分けて使うこと。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------Iris（3種類全ての目的変数を使用）---------\n",
    "ポイントはone-hotにしないといけない。  \n",
    "２値分類は出力層が１つにできるが、３分類からはone-hotにしなければいけない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 4) (24, 4) (30, 4)\n",
      "(96, 1) (24, 1) (30, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを3値分類するコード\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# データセットの読み込み\n",
    "dataset_path =\"/Users/tsuneo/kaggle/IrisSpecies/Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "#df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y[y=='Iris-setosa'] = 2\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape    　　　　　　　　　　　　　　　　:    (96, 1)\n",
      "y_train_one_hot.shape    :    (96, 3)\n",
      "y_train_one_hot.型  　　　　　　  :    float64\n",
      "------\n",
      "y_test.shape  　　　　　　　　　　　　　　　　  :    (30, 1)\n",
      "y_test_one_hot.shape    :    (30, 3)\n",
      "y_test_one_hot.型　　　　　　    :    float64\n",
      "------\n",
      "y_val.shape  　　　　　　　　　　　　　　　　  :    (24, 1)\n",
      "y_val_one_hot.shape    :    (24, 3)\n",
      "y_val_one_hot.型　　　　　　    :    float64\n"
     ]
    }
   ],
   "source": [
    "#  one-hot表現にする\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# 教師ラベルだけ変換する\n",
    "#y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "#y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "#y_val_one_hot = enc.transform(y_val[:, np.newaxis])\n",
    "y_train_one_hot = enc.fit_transform(y_train)\n",
    "y_test_one_hot = enc.transform(y_test)\n",
    "y_val_one_hot = enc.transform(y_val)\n",
    "\n",
    "# shapeの確認\n",
    "print(\"y_train.shape    　　　　　　　　　　　　　　　　:   \",y_train.shape)\n",
    "print(\"y_train_one_hot.shape    :   \",y_train_one_hot.shape)\n",
    "print(\"y_train_one_hot.型  　　　　　　  :   \",y_train_one_hot.dtype)\n",
    "print(\"------\")\n",
    "print(\"y_test.shape  　　　　　　　　　　　　　　　　  :   \",y_test.shape)\n",
    "print(\"y_test_one_hot.shape    :   \",y_test_one_hot.shape)\n",
    "print(\"y_test_one_hot.型　　　　　　    :   \",y_test_one_hot.dtype)\n",
    "print(\"------\")\n",
    "print(\"y_val.shape  　　　　　　　　　　　　　　　　  :   \",y_val.shape)\n",
    "print(\"y_val_one_hot.shape    :   \",y_val_one_hot.shape)\n",
    "print(\"y_val_one_hot.型　　　　　　    :   \",y_val_one_hot.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 377.51, val_loss : 333.63, acc : 0.67, val_acc : 0.71\n",
      "Epoch 1, loss : 338.24, val_loss : 297.11, acc : 0.67, val_acc : 0.71\n",
      "Epoch 2, loss : 298.80, val_loss : 260.48, acc : 0.67, val_acc : 0.71\n",
      "Epoch 3, loss : 259.36, val_loss : 223.79, acc : 0.67, val_acc : 0.71\n",
      "Epoch 4, loss : 220.02, val_loss : 187.04, acc : 0.67, val_acc : 0.71\n",
      "Epoch 5, loss : 180.73, val_loss : 150.28, acc : 0.67, val_acc : 0.71\n",
      "Epoch 6, loss : 141.52, val_loss : 113.46, acc : 0.67, val_acc : 0.71\n",
      "Epoch 7, loss : 102.23, val_loss : 76.55, acc : 0.67, val_acc : 0.71\n",
      "Epoch 8, loss : 62.85, val_loss : 39.60, acc : 0.67, val_acc : 0.71\n",
      "Epoch 9, loss : 33.94, val_loss : 18.85, acc : 0.58, val_acc : 0.50\n",
      "Epoch 10, loss : 22.39, val_loss : 24.64, acc : 0.50, val_acc : 0.58\n",
      "Epoch 11, loss : 18.29, val_loss : 19.11, acc : 0.67, val_acc : 0.62\n",
      "Epoch 12, loss : 21.23, val_loss : 12.95, acc : 0.58, val_acc : 0.54\n",
      "Epoch 13, loss : 21.76, val_loss : 11.50, acc : 0.58, val_acc : 0.58\n",
      "Epoch 14, loss : 19.76, val_loss : 10.90, acc : 0.58, val_acc : 0.58\n",
      "Epoch 15, loss : 16.39, val_loss : 10.87, acc : 0.58, val_acc : 0.58\n",
      "Epoch 16, loss : 13.84, val_loss : 11.39, acc : 0.67, val_acc : 0.62\n",
      "Epoch 17, loss : 12.83, val_loss : 10.88, acc : 0.67, val_acc : 0.62\n",
      "Epoch 18, loss : 12.48, val_loss : 9.69, acc : 0.67, val_acc : 0.62\n",
      "Epoch 19, loss : 11.90, val_loss : 8.78, acc : 0.67, val_acc : 0.62\n",
      "test_acc : 0.47\n"
     ]
    }
   ],
   "source": [
    "class GetMiniBatch2:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._counter = 0\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            self._counter = 0\n",
    "            raise StopIteration()\n",
    "\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.0002\n",
    "batch_size = 12\n",
    "num_epochs = 20\n",
    "\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch2(X_train, y_train_one_hot, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # 全結合層っぽい\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    \n",
    "    # 活性化関数reluのようだ\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # 全結合\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    \n",
    "    # 活性化関数relu\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # 全結合のままで出力しているので、恒等関数で対応している意味合い？\n",
    "    # →違う。出力層の活性化関数はsigmoidである。誤差逆伝搬のコードは★１\n",
    "    # 誤差逆伝搬ではない、予測値を算出する処理は★2\n",
    "    # つまり、誤差逆伝搬の「計算グラフ」では、★1と★２に枝分かれしている\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数\n",
    "# reduce_meanは、np.meanと等価のようだ\n",
    "# シグモイド＋交差エントロピーの組み合わせ.サンプル毎の損失をだすため、reduce_meanしている\n",
    "# ★１\n",
    "# ◼️◼️◼️◼️◼️ループで呼び出している箇所\n",
    "#loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "\n",
    "# 最適化手法\n",
    "# AdamOptimizer　：Adamアルゴリズムを実装するオプティマイザ\n",
    "# 重みを更新する処理\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# ◼️◼️◼️◼️◼️ループで呼び出している箇所\n",
    "# loss更新によって最小化する操作を追加し、var_list\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果 -0.5は閾値　★2\n",
    "#tmp1 = tf.sign(Y - 0.5) # debug\n",
    "#tmp2 = tf.sigmoid(logits)\n",
    "#correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "#correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.nn.softmax(logits) - 0.5))\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
    "\n",
    "# 指標値計算\n",
    "# ◼️◼️◼️◼️◼️ループで呼び出している箇所\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    # このrunでは、initだけを実行している。\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        #print(X_train.shape[])\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        \n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            \n",
    "            # ミニバッチごとにループ\n",
    "            # \n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #aa  = sess.run(correct_pred, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #bb = sess.run(correct_pred, feed_dict={X: mini_batch_x, Y: mini_batch_y}) #debug\n",
    "            #print(acc)\n",
    "            #cc = sess.run(tmp2, feed_dict={X: mini_batch_x, Y: mini_batch_y})  #debug\n",
    "            #print(\"correct_pred\", bb.dtype)\n",
    "            #print(\"tf.sigmoid(logits).shape\",cc.shape)\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
    "        print(\"Epoch {}, loss : {:.2f}, val_loss : {:.2f}, acc : {:.2f}, val_acc : {:.2f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    print(\"test_acc : {:.2f}\".format(test_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"DeepPink\">回帰問題ではaccuracyを見てはいけない（分類とは違い、当たることは無いので）。そのため、lossが下がっていることが確認できれば良い。<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------MNIST---------\n",
    "ニューラルネットワークをスクラッチで作成した際に使用したデータセットを使う。以前ダウンロードしたときのpathを指定すると良い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44800, 784) (11200, 784) (14000, 784)\n",
      "(44800,) (11200,) (14000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from  sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# 保存先を指定\n",
    "mnist_dir = \"./mnist_data/\"\n",
    "\n",
    "# MNISTの読み込み\n",
    "mnist = fetch_mldata('MNIST original', data_home=mnist_dir)\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.target, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.0\n",
      "0.0\n",
      "----\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# ラベルをint型にしておく\n",
    "y_train = y_train.astype(np.int)\n",
    "y_test = y_test.astype(np.int)\n",
    "y_val = y_val.astype(np.int)\n",
    "\n",
    "# 特徴量だけを、float型へ変換している\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_val = X_val.astype(np.float)\n",
    "\n",
    "# 最大値・最小値の確認（処理前）\n",
    "print(X_train.max())\n",
    "print(X_train.min())\n",
    "print(\"----\")\n",
    "\n",
    "# 正則化？正規化？標準化？している\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_val /= 255\n",
    "\n",
    "# 最大値・最小値の確認（処理前）\n",
    "print(X_train.max())\n",
    "print(X_train.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((44800,), (14000,), (11200,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape    　　　　　　　　　　　　　　　　:    (44800,)\n",
      "y_train_one_hot.shape    :    (44800, 10)\n",
      "y_train_one_hot.型  　　　　　　  :    float64\n",
      "------\n",
      "y_test.shape  　　　　　　　　　　　　　　　　  :    (14000,)\n",
      "y_test_one_hot.shape    :    (14000, 10)\n",
      "y_test_one_hot.型　　　　　　    :    float64\n",
      "------\n",
      "y_val.shape  　　　　　　　　　　　　　　　　  :    (11200,)\n",
      "y_val_one_hot.shape    :    (11200, 10)\n",
      "y_val_one_hot.型　　　　　　    :    float64\n"
     ]
    }
   ],
   "source": [
    "#  one-hot表現にする\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# 教師ラベルだけ変換する\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "y_val_one_hot = enc.transform(y_val[:, np.newaxis])\n",
    "\n",
    "# shapeの確認\n",
    "print(\"y_train.shape    　　　　　　　　　　　　　　　　:   \",y_train.shape)\n",
    "print(\"y_train_one_hot.shape    :   \",y_train_one_hot.shape)\n",
    "print(\"y_train_one_hot.型  　　　　　　  :   \",y_train_one_hot.dtype)\n",
    "print(\"------\")\n",
    "print(\"y_test.shape  　　　　　　　　　　　　　　　　  :   \",y_test.shape)\n",
    "print(\"y_test_one_hot.shape    :   \",y_test_one_hot.shape)\n",
    "print(\"y_test_one_hot.型　　　　　　    :   \",y_test_one_hot.dtype)\n",
    "print(\"------\")\n",
    "print(\"y_val.shape  　　　　　　　　　　　　　　　　  :   \",y_val.shape)\n",
    "print(\"y_val_one_hot.shape    :   \",y_val_one_hot.shape)\n",
    "print(\"y_val_one_hot.型　　　　　　    :   \",y_val_one_hot.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 1059.4, val_loss : 1097.4, acc : 0.19, val_acc : 0.18\n",
      "Epoch 1, loss : 698.0, val_loss : 688.1, acc : 0.25, val_acc : 0.26\n",
      "Epoch 2, loss : 488.5, val_loss : 477.3, acc : 0.38, val_acc : 0.37\n",
      "Epoch 3, loss : 374.2, val_loss : 363.0, acc : 0.46, val_acc : 0.46\n",
      "Epoch 4, loss : 297.2, val_loss : 289.2, acc : 0.54, val_acc : 0.53\n",
      "Epoch 5, loss : 243.6, val_loss : 239.7, acc : 0.58, val_acc : 0.59\n",
      "Epoch 6, loss : 203.9, val_loss : 205.0, acc : 0.61, val_acc : 0.63\n",
      "Epoch 7, loss : 175.2, val_loss : 179.6, acc : 0.64, val_acc : 0.67\n",
      "Epoch 8, loss : 152.7, val_loss : 160.2, acc : 0.68, val_acc : 0.69\n",
      "Epoch 9, loss : 136.2, val_loss : 144.9, acc : 0.70, val_acc : 0.72\n",
      "Epoch 10, loss : 124.2, val_loss : 132.6, acc : 0.73, val_acc : 0.74\n",
      "Epoch 11, loss : 113.7, val_loss : 122.5, acc : 0.74, val_acc : 0.75\n",
      "Epoch 12, loss : 104.4, val_loss : 114.1, acc : 0.76, val_acc : 0.76\n",
      "Epoch 13, loss : 96.3, val_loss : 107.0, acc : 0.76, val_acc : 0.78\n",
      "Epoch 14, loss : 88.5, val_loss : 100.9, acc : 0.78, val_acc : 0.79\n",
      "Epoch 15, loss : 81.4, val_loss : 95.6, acc : 0.79, val_acc : 0.80\n",
      "Epoch 16, loss : 75.2, val_loss : 91.0, acc : 0.80, val_acc : 0.80\n",
      "Epoch 17, loss : 69.4, val_loss : 86.7, acc : 0.81, val_acc : 0.81\n",
      "Epoch 18, loss : 64.2, val_loss : 82.9, acc : 0.82, val_acc : 0.82\n",
      "Epoch 19, loss : 59.8, val_loss : 79.6, acc : 0.84, val_acc : 0.82\n",
      "test_acc : 0.830\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class GetMiniBatch3:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._counter = 0\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            self._counter = 0\n",
    "            raise StopIteration()\n",
    "\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.00005\n",
    "batch_size = 200\n",
    "num_epochs = 20\n",
    "\n",
    "n_hidden1 = 400\n",
    "n_hidden2 = 200\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train3 = GetMiniBatch3(X_train, y_train_one_hot, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # 全結合\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    \n",
    "    # 活性化関数relu\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # 全結合\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    \n",
    "    # 活性化関数relu\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # 全結合のままで出力しているので、恒等関数で対応している意味合い？\n",
    "    # →違う。出力層の活性化関数はsigmoidである。誤差逆伝搬のコードは★１\n",
    "    # 誤差逆伝搬ではない、予測値を算出する処理は★2\n",
    "    # つまり、誤差逆伝搬の「計算グラフ」では、★1と★２に枝分かれしている\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "#print(tf.shape(logits))\n",
    "# 目的関数\n",
    "# reduce_meanは、np.meanと等価のようだ\n",
    "# シグモイド＋交差エントロピーの組み合わせ.サンプル毎の損失をだすため、reduce_meanしている\n",
    "# ★１\n",
    "# ◼️◼️◼️◼️◼️ループで呼び出している箇所\n",
    "#loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "\n",
    "# 最適化手法\n",
    "# AdamOptimizer　：Adamアルゴリズムを実装するオプティマイザ\n",
    "# 重みを更新する処理\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# ◼️◼️◼️◼️◼️ループで呼び出している箇所\n",
    "# loss更新によって最小化する操作を追加し、var_list\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果 -0.5は閾値　★2\n",
    "#tmp1 = tf.sign(Y - 0.5) # debug\n",
    "#tmp2 = tf.sigmoid(logits)\n",
    "#correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "#correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.nn.softmax(logits) - 0.5))\n",
    "correct_pred = tf.equal(tf.argmax(Y,1), tf.argmax(logits,1))\n",
    "#a = np.argmax(Y)\n",
    "# 指標値計算\n",
    "# ◼️◼️◼️◼️◼️ループで呼び出している箇所\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    # このrunでは、initだけを実行している。\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        #print(X_train.shape[])\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        \n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train3):\n",
    "            \n",
    "            # ミニバッチごとにループ\n",
    "            #print(mini_batch_x.shape)\n",
    "            #print(mini_batch_y.shape)\n",
    "            #print(mini_batch_x.shape)\n",
    "            #a = sess.run(a, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #print(a)\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #aa  = sess.run(correct_pred, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #bb = sess.run(correct_pred, feed_dict={X: mini_batch_x, Y: mini_batch_y}) #debug\n",
    "            #print(aa)\n",
    "            #cc = sess.run(tmp2, feed_dict={X: mini_batch_x, Y: mini_batch_y})  #debug\n",
    "            #print(\"correct_pred\", bb.dtype)\n",
    "            #print(\"tf.sigmoid(logits).shape\",cc.shape)\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
    "        print(\"Epoch {}, loss : {:.1f}, val_loss : {:.1f}, acc : {:.2f}, val_acc : {:.2f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------House Prices---------\n",
    "この中のtrain.csvをダウンロードし、目的変数としてSalePrice、説明変数として、GrLivAreaとYearBuiltを使う。説明変数はさらに増やしても良い。\n",
    "\n",
    "前の2つと異なり回帰問題である。分類問題と回帰問題の違いを理解している必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(934, 2) (234, 2) (292, 2)\n",
      "(934, 1) (234, 1) (292, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"/Users/tsuneo/kaggle/houseprice/train.csv\")\n",
    "X = data[[\"GrLivArea\", \"YearBuilt\"]].values\n",
    "y = data[\"SalePrice\"].values\n",
    "\n",
    "#print(X.shape)\n",
    "y = y[:, np.newaxis]\n",
    "# 正規化\n",
    "X = np.log(X)\n",
    "y = np.log(y)\n",
    "\n",
    "from  sklearn.model_selection import train_test_split\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 390.79, val_loss : 397.32, acc : 0.33, val_acc : -0.11\n",
      "Epoch 1, loss : 244.60, val_loss : 240.35, acc : 15.48, val_acc : 15.15\n",
      "Epoch 2, loss : 170.14, val_loss : 172.10, acc : 11.62, val_acc : 11.23\n",
      "Epoch 3, loss : 125.59, val_loss : 124.08, acc : 12.37, val_acc : 12.00\n",
      "Epoch 4, loss : 92.38, val_loss : 86.90, acc : 12.64, val_acc : 12.28\n",
      "Epoch 5, loss : 63.39, val_loss : 56.93, acc : 13.01, val_acc : 12.60\n",
      "Epoch 6, loss : 42.24, val_loss : 33.20, acc : 13.05, val_acc : 12.80\n",
      "Epoch 7, loss : 23.03, val_loss : 17.58, acc : 13.41, val_acc : 12.90\n",
      "Epoch 8, loss : 12.88, val_loss : 10.50, acc : 13.48, val_acc : 12.75\n",
      "Epoch 9, loss : 9.68, val_loss : 8.50, acc : 13.37, val_acc : 12.56\n",
      "Epoch 10, loss : 8.59, val_loss : 7.97, acc : 13.35, val_acc : 12.51\n",
      "Epoch 11, loss : 7.94, val_loss : 7.62, acc : 13.29, val_acc : 12.45\n",
      "Epoch 12, loss : 7.52, val_loss : 7.27, acc : 13.26, val_acc : 12.43\n",
      "Epoch 13, loss : 7.13, val_loss : 6.89, acc : 13.24, val_acc : 12.42\n",
      "Epoch 14, loss : 6.76, val_loss : 6.51, acc : 13.21, val_acc : 12.41\n",
      "Epoch 15, loss : 6.39, val_loss : 6.13, acc : 13.19, val_acc : 12.39\n",
      "Epoch 16, loss : 5.95, val_loss : 5.72, acc : 13.14, val_acc : 12.36\n",
      "Epoch 17, loss : 5.34, val_loss : 5.29, acc : 13.10, val_acc : 12.31\n",
      "Epoch 18, loss : 4.77, val_loss : 4.86, acc : 13.08, val_acc : 12.29\n",
      "Epoch 19, loss : 4.11, val_loss : 4.40, acc : 13.03, val_acc : 12.26\n",
      "test_acc : 11.948\n"
     ]
    }
   ],
   "source": [
    "class GetMiniBatch4:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._counter = 0\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            self._counter = 0\n",
    "            raise StopIteration()\n",
    "\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.0001\n",
    "batch_size = 20\n",
    "num_epochs = 20\n",
    "\n",
    "n_hidden1 = 400\n",
    "n_hidden2 = 200\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train4 = GetMiniBatch4(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # 全結合層っぽい\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    \n",
    "    # 活性化関数reluのようだ\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # 全結合\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    \n",
    "    # 活性化関数relu\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # 全結合のままで出力しているので、恒等関数で対応している意味合い？\n",
    "    # →違う。出力層の活性化関数はsigmoidである。誤差逆伝搬のコードは★１\n",
    "    # 誤差逆伝搬ではない、予測値を算出する処理は★2\n",
    "    # つまり、誤差逆伝搬の「計算グラフ」では、★1と★２に枝分かれしている\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数\n",
    "# reduce_meanは、np.meanと等価のようだ\n",
    "# シグモイド＋交差エントロピーの組み合わせ.サンプル毎の損失をだすため、reduce_meanしている\n",
    "# ★１\n",
    "# ◼️◼️◼️◼️◼️ループで呼び出している箇所\n",
    "#loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "#loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# 出力層に活性化関数は通さない（恒等関数）\n",
    "loss_op = tf.reduce_mean(tf.losses.mean_squared_error(labels=Y, predictions=logits))\n",
    "\n",
    "# 最適化手法\n",
    "# AdamOptimizer　：Adamアルゴリズムを実装するオプティマイザ\n",
    "# 重みを更新する処理\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# ◼️◼️◼️◼️◼️ループで呼び出している箇所\n",
    "# loss更新によって最小化する操作を追加し、var_list\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "# 推定結果 -0.5は閾値　★2\n",
    "#tmp1 = tf.sign(Y - 0.5) # debug\n",
    "#tmp2 = tf.sigmoid(logits)\n",
    "#correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "#correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.nn.softmax(logits) - 0.5))\n",
    "correct_pred = logits\n",
    "\n",
    "# 指標値計算\n",
    "# ◼️◼️◼️◼️◼️ループで呼び出している箇所\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    # このrunでは、initだけを実行している。\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        #print(X_train.shape[])\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        \n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train4):\n",
    "            \n",
    "            # ミニバッチごとにループ\n",
    "            #print(mini_batch_x.shape)\n",
    "            #print(mini_batch_y.shape)\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #aa  = sess.run(correct_pred, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #bb = sess.run(correct_pred, feed_dict={X: mini_batch_x, Y: mini_batch_y}) #debug\n",
    "            #print(acc)\n",
    "            #cc = sess.run(tmp2, feed_dict={X: mini_batch_x, Y: mini_batch_y})  #debug\n",
    "            #print(\"correct_pred\", bb.dtype)\n",
    "            #print(\"tf.sigmoid(logits).shape\",cc.shape)\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.2f}, val_loss : {:.2f}, acc : {:.2f}, val_acc : {:.2f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"DeepPink\">回帰問題ではaccuracyを見てはいけない（分類とは違い、当たることは無いので）。そのため、lossが下がっていることが確認できれば良い。<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.（オプション）他のフレームワークやAPIでの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras**  \n",
    "Kerasには2種類のAPI（記述方法）がある。Kerasのドキュメントは日本語が充実しているので読み進めやすい。\n",
    "「Sequential API」\n",
    "\n",
    "**Chainer**\n",
    "\n",
    "**get_mnist()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※今回は実装しませんでした。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
