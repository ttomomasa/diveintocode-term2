{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題提出時（2018/12/19）の所感\n",
    "初めて自然言語に取り組みました。新しい用語ばかりなのでその意味を理解するのに時間を費やしました。  \n",
    "＜実施できていなこと＞  \n",
    "（オプション）Keras実装\n",
    "（オプション）日本語への適用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.この課題の目的\n",
    "・自然言語の機械学習での扱われ方を学ぶ  \n",
    "・自然言語のベクトル化を学ぶ  \n",
    "\n",
    "【目的としないこと】  \n",
    "・分類など具体的なタスクの扱い方  \n",
    "\n",
    "# 2.進め方\n",
    "・自然言語のベクトル化を行う（1日目〜2日目）  \n",
    "・手法の説明を記述する。（2日目夜発表）  \n",
    "\n",
    "# 3.word2vec\n",
    "**実行**  \n",
    "自然言語処理概要1から4のテキストを読み進めた上で、以下のツールを動かす。自然言語処理概要4の中に解説があるので、それに沿えば良い。  \n",
    "https://github.com/svn2github/word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実行結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新たなデータへの適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "My name is Tsuneo Tomomasa. I have a dog who name is Fuuga.\n",
      "IGO is very difficult game in the world.\n"
     ]
    }
   ],
   "source": [
    "# ファイルの確認\n",
    "with open(\"tomomasa\") as f:\n",
    "    s = f.read()\n",
    "    print(type(s))\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file tomomasa\n",
      "Vocab size: 1\n",
      "Words in train file: 1\n"
     ]
    }
   ],
   "source": [
    "# 新たなデータへの適用\n",
    "!./word2vec -train tomomasa -output vectors_tomo4.txt -sg 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 20 -text 1 -iter 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "1 200\n",
      "</s> 0.002001 0.002210 -0.001915 -0.001639 0.000683 0.001511 0.000470 0.000106 -0.001802 0.001109 -0.002178 0.000625 -0.000376 -0.000479 -0.001658 -0.000941 0.001290 0.001513 0.001485 0.000799 0.000772 -0.001901 -0.002048 0.002485 0.001901 0.001545 -0.000302 0.002008 -0.000247 0.000367 -0.000075 -0.001492 0.000656 -0.000669 -0.001913 0.002377 0.002190 -0.000548 -0.000113 0.000255 -0.001819 -0.002004 0.002277 0.000032 -0.001291 -0.001521 -0.001538 0.000848 0.000101 0.000666 -0.002107 -0.001904 -0.000065 0.000572 0.001275 -0.001585 0.002040 0.000463 0.000560 -0.000304 0.001493 -0.001144 -0.001049 0.001079 -0.000377 0.000515 0.000902 -0.002044 -0.000992 0.001457 0.002116 0.001966 -0.001523 -0.001054 -0.000455 0.001001 -0.001894 0.001499 0.001394 -0.000799 -0.000776 -0.001119 0.002114 0.001956 -0.000590 0.002107 0.002410 0.000908 0.002491 -0.001556 -0.000766 -0.001054 -0.001454 0.001407 0.000790 0.000212 -0.001097 0.000762 0.001530 0.000097 0.001140 -0.002476 0.002157 0.000240 -0.000916 -0.001042 -0.000374 -0.001468 -0.002185 -0.001419 0.002139 -0.000885 -0.001340 0.001159 -0.000852 0.002378 -0.000802 -0.002294 0.001358 -0.000037 -0.001744 0.000488 0.000721 -0.000241 0.000912 -0.001979 0.000441 0.000908 -0.001505 0.000071 -0.000030 -0.001200 -0.001416 -0.002347 0.000011 0.000076 0.000005 -0.001967 -0.002481 -0.002373 -0.002163 -0.000274 0.000696 0.000592 -0.001591 0.002499 -0.001006 -0.000637 -0.000702 0.002366 -0.001882 0.000581 -0.000668 0.001594 0.000020 0.002135 -0.001410 -0.001303 -0.002096 -0.001833 -0.001600 -0.001557 0.001222 -0.000933 0.001340 0.001845 0.000678 0.001475 0.001238 0.001170 -0.001775 -0.001717 -0.001828 -0.000066 0.002065 -0.001368 -0.001530 -0.002098 0.001653 -0.002089 -0.000290 0.001089 -0.002309 -0.002239 0.000721 0.001762 0.002132 0.001073 0.001581 -0.001564 -0.001820 0.001987 -0.001382 0.000877 0.000287 0.000895 -0.000591 0.000099 -0.000843 -0.000563 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 作成されたファイルの中身を確認してみる\n",
    "with open(\"vectors_tomo4.txt\") as f:\n",
    "    s = f.read()\n",
    "    print(type(s))\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自然言語処理概要4のtext8を実行した\n",
    "vectors.binファイル（57.7MB）が作成された"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file text8\n",
      "Vocab size: 71291\n",
      "Words in train file: 16718843\n",
      "Alpha: 0.000005  Progress: 100.10%  Words/thread/sec: 108.95k  "
     ]
    }
   ],
   "source": [
    "!./word2vec -train text8 -output vectors.bin -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 20 -binary 1 -iter 15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.説明\n",
    "以下の内容についてJupyter Notebookにまとめる。2日目の夜には自分の言葉で伝えられるようにしておく。\n",
    "\n",
    "#### ・自然言語処理の分野ではどのような応用事例があるか\n",
    "**【回答】**  \n",
    "・文書分類（document categorization）  \n",
    "・機械翻訳（machine translation）  \n",
    "・文書要約（text summarization）  \n",
    "・質問応答（question answering）  \n",
    "・対話（dialog）  \n",
    "・感情分析  \n",
    "・筆者識別  \n",
    "\n",
    "#### ・なぜ自然言語をベクトルの形にするのか\n",
    "**【回答】**  \n",
    "・ベクトルの形にしないと、ニューラルネットワークでは扱えないからです。  \n",
    "・単語や文字といった「記号の世界」からニューラルネットワークが主に扱うベクトルや行列といった「実数値連続領域の世界」へ変換する必要があるからです。また、ニューラルネットワークで処理した結果を、最終的には「記号の世界」へ戻す必要があります。\n",
    "\n",
    "\n",
    "#### ・分布仮説とは何か\n",
    "**【回答】**  \n",
    "分布仮説（distributional hypothesis）とは、「単語の意味は、周囲の単語によって形成される」という考え方です。\n",
    "単語自体には意味がなく、その単語の「コンテキスト（文脈）」によって、単語の意味が形成されるという考え方です。\n",
    "\n",
    "\n",
    "#### ・word2vecにはskip-gramとCBOWの2つの方法があるがどのようなものか。どう選択すれば良いのか\n",
    "**【回答】**  \n",
    "・CBOW（continuous bag-of-words）は、コンテキストからターゲットを推測することを目的としたニューラルネットワークです。（「ターゲット」は中央の単語、その周囲の単語が「コンテキスト」です）  \n",
    "・skip-gramはCBOWで扱うコンテキストとターゲットを逆転させたモデルです。中央の単語（ターゲット）から、周囲の複数ある単語（コンテキスト）を推測します。  \n",
    "\n",
    "どう選択すれば良いかについては、「skip-gramモデル」を選択すべきとなります。理由は、単語の分散表現の精度の点において、多くの場合、skip-gramモデルの方が良い結果が得られるからです。\n",
    "\n",
    "\n",
    "#### ・カウントベースと呼ばれる手法もあるがどういったものか。利用する上でどのような違いがあるか\n",
    "**【回答】**  \n",
    "・単語をベクトルで表す手法として、「カウントベース」と「推論ベース」の方法に分けられます。推論ベースの手法はword2vecの仕組みがあります。どちらの手法も、背景には分布仮説があります。\n",
    "\n",
    "・カウントベースと推論ベースの違いですが、カウントベースの手法は、コーパスの全体の統計データから１回の学習で単語の分散表現を得ます。一方、推論ベースでは、コーパスの一部を何度も見ながら学習します（ミニバッチ学習）。語彙に新しい単語を追加するケースで、単語の分散表現の更新作業が発生した場合を考えます。この時、カウントベースの手法ではもう一度ゼロから計算を行う必要があります。それに対して、推論ベースの手法（word2vec）は、パラメータの再学習が行えます。これまでに学習した重みを初期値として再学習することで、その学習した経験を損なわずに、単語の分散表現の更新が効率的に行えます。この点において、推論ベースの手法の方が利用する上でメリットがあると思います。  \n",
    " しかし、単語の分散表現の性質や精度については、どちらの手法も優劣はつけられないことが報告されているようです。\n",
    " \n",
    "※分散表現：「単語の意味」を的確に捉えたベクトル表現\n",
    "\n",
    "#### ・word2vecなどの手法に入力する前にどのような前処理があるか。英語の場合と日本語の場合を考える\n",
    "**【回答】**  \n",
    "①コーパスからコンテキストとターゲットを作成する  \n",
    "②コンテキストとターゲットをone-hot表現に変換する  \n",
    "\n",
    "・日本語の場合：「わかち書き」「形態素解析」  \n",
    "・英語の場合：英語はスペースで区切られているので、日本語で必要な上記の処理は不要"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
