{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.この課題の目的\n",
    "・ここまでの内容を復習する  \n",
    "・総合的な深層学習の知識を確認する\n",
    "\n",
    "### 2.進め方\n",
    "・これまでのsprintで合格していないものに取り組む（1日目〜2日目）  \n",
    "・合格した課題の完成度を上げる（1日目〜2日目）  \n",
    "・深層学習の総合的な質問に対する説明を記述する（2日目夜発表）  \n",
    "\n",
    "### 3.課題の修正\n",
    "これまでのsprintで合格していない課題がある場合はそれに取り組むこと。全ての課題に合格することがDIVE INTO CODEを卒業するための必須条件である。\n",
    "\n",
    "合格しているものに関しても改善できる部分がある場合は取り組むこと。\n",
    "\n",
    "## 4.総合的な質問\n",
    "### ・深層学習とは何か  \n",
    " **【回答】**  \n",
    "・一言でいうと関数である。入力値を受け取ったら何らかの処理を施して出力する関数である。  \n",
    "・学習とは、ある一定のデータセット（教師データ）を元に、その関数が保持するパラメータ（重み）を調整する作業である。その調整する指標となるものが損失関数である。この損失関数の値を小さくするように重みパラメータを最適化手法（オプティマイザー）により更新する作業を学習という。  \n",
    "・つまり、深層学習とは、入力値を多段の隠れ層（Layer）を通して、畳み込み・プーリング・全結合・活性化関数・誤差逆伝播などの処理を施して学習することと、学習した重みパラメータを利用して結果を出力するネットワーク状の関数のことである。  \n",
    "\n",
    "\n",
    "### ・深層学習では何ができるのか  \n",
    " **【回答】**  \n",
    "・分類や回帰問題を解くことができる  \n",
    "・画像認識、物体検出  \n",
    "・ゲームコンピュータ（囲碁、将棋）を強くするための手法  \n",
    "・音声認識、自然言語処理にも応用されている  \n",
    "・強化学習と絡めた、深層強化学習が今ホットである  \n",
    "・GANで画像データの生成  \n",
    " \n",
    "### ・他の機械学習手法よりも深層学習が適しているデータや状況は何か  \n",
    " **【回答】**  \n",
    "・潤沢なGPU環境が用意されている状況  \n",
    "・大量なデータセットが用意されている状況\n",
    "\n",
    "### ・深層学習の層の組み方はどのように決定するべきか  \n",
    " **【回答】**  \n",
    "・解きたい課題により設計するのだと思います。  \n",
    "・形状が（samples, features）の二次元テンソルに格納された単純なベクトルデータは、多くの場合、密結合された層（densely connected layer）によって処理されます。これらの層は全結合（fully connected layer）とも呼ばれ、KerasではDenseクラスとして定義されています。  \n",
    "・形状が（samples, timesteps, features）の３次元テンソルに格納されたシーケンスデータは、大抵LSTM層などのリカレント層（recurrent layer）によって処理されます。  \n",
    "・４次元テンソルに格納された画像データは、通常は２次元の畳み込み（Conv2D）によって処理されます。\n",
    "\n",
    "\n",
    "### ・活性化関数はどのように選択すれば良いか  \n",
    " **【回答】**  \n",
    " 勾配消失（gradient vanishing）しないように選択する。ただ、今はLeRUが通常利用されているようです。\n",
    "\n",
    "\n",
    "### ・学習曲線を確認して学習データと検証データの損失の差が大きかった場合にはまず何をすれば良いか  \n",
    " **【回答】**  \n",
    " 過学習が起きていると思われるので、以下の対応が考えられると思います。  \n",
    " ・ドロップアウトの追加する  \n",
    " ・データ数を増やす（Data Augmentation（左右・上下反転など））  \n",
    " ・ネットワークのサイズを削減する（重みパラメータの数を減らす）  \n",
    " ・荷重減衰（Weight decay）を使う  \n",
    " 　　荷重減衰とは、大きな重みを持つことに対してペナルティーを課すことです。そもそも過学習は、重みパラメータが大きな値を取ることによって発生することが多くあるため。  \n",
    "\n",
    "### ・深層学習はデータ量がある程度必要だとされているがそれはなぜか  \n",
    " **【回答】**  \n",
    "更新する重みパラメータの数が膨大にあるため。\n",
    "深層学習の基本的な特徴の１つは、特徴エンジニアリングを手作業で行わなくても、訓練データからその興味深い特徴量を見つけ出せることです。それが可能となるのは訓練データが大量にある場合だけであるからです。\n",
    "\n",
    "### ・手元にあるデータが少ない場合はどうするか  \n",
    " **【回答】**  \n",
    "・転移学習（ファインチューニング）する  \n",
    "・データ数を増やす（Data Augmentation（左右・上下反転など））  \n",
    " ・ネットワークのサイズを削減する（重みパラメータの数を減らす）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
